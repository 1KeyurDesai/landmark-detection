{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac2cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Landmark Detection using CNN (Kaggle Dataset)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Kaggle setup for downloading dataset\n",
    "!pip install kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!echo '{\"username\":\"your_kaggle_username\",\"key\":\"your_kaggle_api_key\"}' > ~/.kaggle/kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Download the Landmark Recognition Dataset from Kaggle\n",
    "!kaggle competitions download -c landmark-recognition-2021\n",
    "!unzip landmark-recognition-2021.zip\n",
    "\n",
    "# Set dataset path\n",
    "dataset_path = 'train'\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Load images and labels (adjust based on dataset structure)\n",
    "for class_id in os.listdir(dataset_path):\n",
    "    class_path = os.path.join(dataset_path, class_id)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "    for img_name in os.listdir(class_path)[:100]:  # Limit to 100 images per class for simplicity\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        try:\n",
    "            img = Image.open(img_path).resize((128, 128))\n",
    "            data.append(np.array(img))\n",
    "            labels.append(class_id)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "data = np.array(data) / 255.0\n",
    "labels = pd.factorize(labels)[0]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# CNN Model\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(len(set(labels)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot accuracy and loss graphs\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model\n",
    "model.save('landmark_detection_model.h5')\n",
    "\n",
    "print(\"Landmark Detection Project Completed!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
